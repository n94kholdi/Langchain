{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt9wg84wNjGfVEwC9ysQuAVdEJlT9bDYjtMeMA8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt9wg84wNjGfVEwC9ysQuAVdEJlT9bDYjtMeMA8g\n"
     ]
    }
   ],
   "source": [
    "cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "print(cohere_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Cohere model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.llms import Cohere\n",
    "\n",
    "# model = Cohere(temperature=0.1)\n",
    "model = Cohere(cohere_api_key=cohere_api_key, model=\"command\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "chat_model = ChatCohere(cohere_api_key=cohere_api_key, model=\"command-r-plus\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='این جمله رو به فرانسوی ترجمه کن: من عاشق برنامه\\u200cنویسی\\u200cام', additional_kwargs={}, response_metadata={}), AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"این جمله رو به فرانسوی ترجمه کن: من عاشق برنامه‌نویسی‌ام\")\n",
    "history.add_ai_message(\"J'adore la programmation.\")\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hi\n",
      "AI: Hello! How can I help you today?\n",
      "Human: what is brother in persian?\n",
      "AI: 'Brother' in Persian is 'برادر' (baráadar).\n",
      "Human: and sister?\n",
      "AI: 'Sister' in Persian is 'خواهر' (khahaar).\n",
      "Human: what was the first word in our chat?\n",
      "AI: The first word in our chat was 'hi'.\n",
      "Chat is done!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "while(True):\n",
    "    input_ = input(\"سوالت را بگو. اگر میخواهی چت متوقف شود، عدد 0 را وارد کن\")\n",
    "    \n",
    "    if input_ != \"0\":\n",
    "        history.add_user_message(input_)\n",
    "        print(f\"Human: {input_}\")\n",
    "\n",
    "        output = \"\"\n",
    "        print(\"AI: \", end=\"\")\n",
    "        for chunk in chat_model.stream(history.messages):\n",
    "            output = output + chunk.content\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "        print()\n",
    "\n",
    "        history.add_ai_message(output)\n",
    "    else:\n",
    "        print(\"Chat is done!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"تو یک مدل گفت‌وگو هستی که مانند دوست با کاربر صحبت می‌کنی.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# A dictionary to store the chat history for each session id\n",
    "store = {}\n",
    "\n",
    "# A function that returns the chat history for a given session id\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # Create a new ChatMessageHistory object and add it to the store\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # Return the message history for the session id# Return the message history for the session id\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history, # function that returns chat history for a session\n",
    "    input_messages_key=\"input\", # where the current user message goes\n",
    "    history_messages_key=\"chat_history\" # where the previous chat history goes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat is done!!!\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "\n",
    "    input_ = input(\"سوالت را بگو. اگر میخواهی چت متوقف شود، عدد 0 را وارد کن\")\n",
    "\n",
    "    if input_ != \"0\":\n",
    "        print(f\"Human: {input_}\")\n",
    "        output = \"\"\n",
    "        print(\"AI: \", end=\"\")\n",
    "        for chunk in with_message_history.stream({\"input\": input_},\n",
    "                                                 {\"configurable\": {\"session_id\": \"123\"}}):\n",
    "            output = output + chunk.content\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Chat is done!!!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['123', '1234'])\n"
     ]
    }
   ],
   "source": [
    "print(store.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Trims the chat history to the last 2 messages\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "\n",
    "chain_with_trimming = (\n",
    "    # Replace the current chat_history with the trimmed chat history\n",
    "    # RunnablePassthrough.assign() is a shortcut to avoid writing custom lambdas or manual pre-processing.\n",
    "    RunnablePassthrough.assign(chat_history=itemgetter(\"chat_history\") | trimmer)\n",
    "    | chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain_with_trimming,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: سلام خوبی؟ من دیبا هستم.  \n",
      "\n",
      "AI: سلام دیبا! خوشحالم که باهات آشنا شدم. من هم خوبم، ممنون که پرسیدی. چطور میتونم کمکت کنم؟\n",
      "\n",
      "User: بیا یک بازی کنیم. من اسم یک فرد مشهور رو میگم تو با حرف آخر اون اسم یک شخص مشهور دیگه رو بگو.  \n",
      "\n",
      "AI: باشه، بازی جالبی به نظر می‌رسه! شروع کن.\n",
      "\n",
      "User: یورگس لانتیموس \n",
      "\n",
      "AI: س: شارلیز ترون\n",
      "\n",
      "User: ون‌گوگ \n",
      "\n",
      "AI: ج: او یک نقاش هلندی بود که آثارش تاثیر زیادی بر هنر مدرن گذاشت. سبک نقاشی او که با رنگ‌های درخشان و قلم‌موهای ضخیم مشخص می‌شد، بسیار متمایز بود و موضوعاتی مانند مناظر طبیعی، پرتره‌ها و طبیعت بی‌جان را به تصویر می‌کشید. ون‌گوگ در طول زندگی خود با مشکلات مالی و روانی دست و پنجه نرم کرد، اما پس از مرگش به یکی از شناخته‌شده‌ترین و محبوب‌ترین هنرمندان جهان تبدیل شد.\n",
      "\n",
      "User: بازی‌مون رو فراموش کردی؟ \n",
      "\n",
      "AI: ج: بله، ببخشید! من یک ربات هوش مصنوعی هستم و گاهی اوقات نیاز به یادآوری دارم. ما داشتیم در مورد هنرمندان صحبت می‌کردیم. می‌خواهید در مورد هنرمند دیگری صحبت کنیم؟\n",
      "\n",
      "User: اسم من هم یادت نیست؟  \n",
      "\n",
      "AI: ج: بله، متاسفم! من یک ربات هوش مصنوعی هستم و نمی‌توانم اطلاعات را مانند انسان‌ها به خاطر بسپارم. اگر دوست داشته باشید، می‌توانید اسم خود را به من بگویید و من سعی می‌کنم آن را به خاطر بسپارم.\n",
      "\n",
      "User: حله پس خدافظ \n",
      "\n",
      "AI: خدانگهدار! خوشحال میشم بازم ببینمتون.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while (True):\n",
    "\n",
    "    # Get user message\n",
    "    user_message = input('پیام خود را بنویسید ... (برای خروج 0 را بزنید)\\n')\n",
    "\n",
    "    # Exit the loop if the user enters '0'\n",
    "    if (user_message == '0'):\n",
    "        break\n",
    "    else:\n",
    "        print('User:', user_message, '\\n')\n",
    "\n",
    "        model_message = ''\n",
    "        print('AI: ', end='')\n",
    "        for chunk in with_message_history.stream({\"input\": user_message},\n",
    "                                                 {\"configurable\": {\"session_id\": \"1234\"}}):\n",
    "            model_message =  model_message + chunk.content\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در خروجی مثال بالا مشاهده می‌کنید که مدل از یک جایی به بعد (هنگام ارسال پیام «ون‌گوگ») دیگر قوانین بازی را فراموش می‌کند زیرا تنها به دو پیام اخیر دسترسی دارد و در این پیام‌ها چیزی درباره‌ی بازی گفته نشده است. علاوه‌براین اسم کاربر را که در پیام نخست ذکر شده به‌خاطر ندارد. بنابراین مشاهده می‌کنید که این رویکرد اگرچه چالش‌های مطرح‌شده در ابتدای درسنامه را رفع می‌کند و در مصرف توکن‌ها نیز صرفه‌جویی می‌کند اما ممکن است از لحاظ عملکرد نیز همواره خوب عمل نکند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":اما همواره در بخش به همه تاریخچه دسترسی داریم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: سلام خوبی؟ من دیبا هستم. \n",
      "AI: سلام دیبا! خوشحالم که باهات آشنا شدم. من هم خوبم، ممنون که پرسیدی. چطور میتونم کمکت کنم؟\n",
      "Human: بیا یک بازی کنیم. من اسم یک فرد مشهور رو میگم تو با حرف آخر اون اسم یک شخص مشهور دیگه رو بگو. \n",
      "AI: باشه، بازی جالبی به نظر می‌رسه! شروع کن.\n",
      "Human: یورگس لانتیموس\n",
      "AI: س: شارلیز ترون\n",
      "Human: ون‌گوگ\n",
      "AI: ج: او یک نقاش هلندی بود که آثارش تاثیر زیادی بر هنر مدرن گذاشت. سبک نقاشی او که با رنگ‌های درخشان و قلم‌موهای ضخیم مشخص می‌شد، بسیار متمایز بود و موضوعاتی مانند مناظر طبیعی، پرتره‌ها و طبیعت بی‌جان را به تصویر می‌کشید. ون‌گوگ در طول زندگی خود با مشکلات مالی و روانی دست و پنجه نرم کرد، اما پس از مرگش به یکی از شناخته‌شده‌ترین و محبوب‌ترین هنرمندان جهان تبدیل شد.\n",
      "Human: بازی‌مون رو فراموش کردی؟\n",
      "AI: ج: بله، ببخشید! من یک ربات هوش مصنوعی هستم و گاهی اوقات نیاز به یادآوری دارم. ما داشتیم در مورد هنرمندان صحبت می‌کردیم. می‌خواهید در مورد هنرمند دیگری صحبت کنیم؟\n",
      "Human: اسم من هم یادت نیست؟ \n",
      "AI: ج: بله، متاسفم! من یک ربات هوش مصنوعی هستم و نمی‌توانم اطلاعات را مانند انسان‌ها به خاطر بسپارم. اگر دوست داشته باشید، می‌توانید اسم خود را به من بگویید و من سعی می‌کنم آن را به خاطر بسپارم.\n",
      "Human: حله پس خدافظ\n",
      "AI: خدانگهدار! خوشحال میشم بازم ببینمتون.\n"
     ]
    }
   ],
   "source": [
    "print(store['1234'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatCohere(model='command-r-plus', temprature=0.2)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"تو یک دستیار گفت‌وگو هستی. تمام سوالات را به بهترین شکل پاسخ بده. تاریخچه‌ی گفت‌وگو شامل خلاصه‌ای از مکالمه با کاربر تا این لحظه نیز در اختیارت قرار گرفته است.\",\n",
    "            # Or something like this in English: \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_message_history = ChatMessageHistory()\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ال نوبت به بخش اصلی یعنی طراحی خلاصه‌ساز تاریخچه می‌رسد. برای این کار تابعی طراحی کرده‌ایم که در آن یک قالب دستور جداگانه برای تولید خلاصه‌ای از پیام‌ها تعریف شده است. در این تابع پیام‌های موجود در تاریخچه (chat_message_history) گرفته شده و درون این قالب قرار می‌گیرد و سپس برای مدل ارسال می‌شود تا خروجی خود (خلاصه‌ی پیام‌ها) را تولید کند. توجه داشته باشید که چون قصد داشتیم در هنگام خلاصه‌سازی از مدلی با قطعیت بیشتر استفاده کنیم دوباره یک ChatCohere ویژه‌ی این کار و با دمای صفر ساخته‌ایم. پس از آن‌که مدل خلاصه‌ای از تاریخچه را تولید کرد ابتدا به‌کمک تابع clear تاریخچه را خالی می‌کنیم و سپس با استفاده از تابع add_message خلاصه‌ی تولیدشده را به تاریخچه اضافه می‌کنیم.\n",
    "\n",
    "جهت طراحی زنجیره‌ی نهایی کافیست ابتدا با استفاده از ماژول RunnablePassthrough ورودی‌ها را به تابع summarize_messages انتقال دهیم تا تابع اجرا شود و به‌کمک دستور assign خروجی را در متغیر chat_history ذخیره کنیم. پس از این‌که گام نخست طی شد، نوبت به اجرای زنجیره‌ی اصلی یعنی همان with_message_history می‌رسد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chat_model_for_history = ChatCohere(model='command-r-plus', temprature=0)\n",
    "\n",
    "def summerize_messages(chain_input):\n",
    "\n",
    "    stored_messages = chat_message_history.messages\n",
    "\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    \n",
    "    summerization_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\n",
    "            \"user\", \"پیام‌های بالا را در یک پیام خلاصه‌شده فشرده کن. تا جایی که می‌تونی تمام جزئیات و اطلاعات خاص و مهم هر پیام رو در این خلاصه نگه دار\"\n",
    "                # Or something like this in English: \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\"\n",
    "        ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Summerizing messages...\")\n",
    "\n",
    "    summerization_chain = summerization_prompt | chat_model_for_history\n",
    "    summery_message = summerization_chain.invoke({\"chat_history\": stored_messages})\n",
    "\n",
    "    print(\"Finished summerization... summary is: \", summery_message.content)\n",
    "\n",
    "    chat_message_history.clear()\n",
    "    chat_message_history.add_message(summery_message)\n",
    "\n",
    "    return True\n",
    "\n",
    "chain_with_summerization = (\n",
    "    RunnablePassthrough.assign(chat_history=summerize_messages)\n",
    "    | with_message_history\n",
    ")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام نیره جان، ممنونم، خوبم. چطور می‌تونم کمکت کنم؟\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_summerization.invoke({\"input\": \"سلام خوبی؟ من نیره هستم.\"},\n",
    "                                           {\"configurable\": {\"session_id\": \"unused\"}},)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='سلام خوبی؟ من نیره هستم.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='سلام نیره جان، ممنونم، خوبم. چطور می\\u200cتونم کمکت کنم؟', additional_kwargs={'id': 'f92b3c54-9223-4901-be16-a5b464d1e02b', 'finish_reason': 'COMPLETE', 'content': 'سلام نیره جان، ممنونم، خوبم. چطور می\\u200cتونم کمکت کنم؟', 'token_count': {'input_tokens': 222.0, 'output_tokens': 23.0}}, response_metadata={'id': 'f92b3c54-9223-4901-be16-a5b464d1e02b', 'finish_reason': 'COMPLETE', 'content': 'سلام نیره جان، ممنونم، خوبم. چطور می\\u200cتونم کمکت کنم؟', 'token_count': {'input_tokens': 222.0, 'output_tokens': 23.0}}, id='run-284b132f-9f4b-4a3b-8202-b7f4850eba75-0', usage_metadata={'input_tokens': 222, 'output_tokens': 23, 'total_tokens': 245})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summerizing messages...\n",
      "Finished summerization... summary is:  سلام، من نیره هستم. چطوری؟\n",
      "چرا برنامه‌نویس‌ها نمی‌تونن دوش بگیرن؟\n",
      "\n",
      "چون به SOAP حساسیت دارن!\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_summerization.invoke(\n",
    "    {\"input\": \"یک جوک خنده‌دار درباره‌ی برنامه‌نویسی میگی؟\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summerizing messages...\n",
      "Finished summerization... summary is:  یک جوک درباره حساسیت برنامه‌نویس‌ها به SOAP\n",
      "بله، شما خودتان را با نام \"کاربر\" به من معرفی کردید.\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_summerization.invoke(\n",
    "    {\"input\": \"یادته من خودم را با چه اسمی بهت معرفی کردم؟\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "خلاصه سازی بهینه و هوشمندانه:\n",
    "\n",
    "شایان ذکر است که ما در مثال این درسنامه در هر بار ارسال درخواست به مدل، یک خلاصه از تاریخچه تهیه کردیم اما شما می‌توانید به‌شکل‌های خلاقانه‌تری نیز این حافظه را پیاده کنید. به‌عنوان مثال می‌توانید پس از هر چند پیام مشخص یا پس از آن‌که تاریخچه طولانی شد به خلاصه‌سازی آن بپردازید. توجه داشته باشید که این امر می‌تواند به کاهش هزینه‌ها نیز کمک کند زیرا در مثال ما، به‌ازای هر سوال کاربر یک درخواست بیشتر به API فرستاده شده که به‌احتمال زیاد هزینه‌ها را افزایش خواهد داد. علاوه‌بر این پیشنهاد می‌کنیم برای درک بهتر این درسنامه، نت‌بوک مربوط به کدها را یک‌بار با فعال‌بودن حالت دیباگ نیز اجرا کرده و مراحل میانی زنجیره‌ها را به‌صورت کامل بررسی کنید."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
